# January 4, 2023

## ‚òÅÔ∏è Google Cloud

Today, I will start working on the [Google Cloud Computing Foundations: Infrastructure in Google Cloud](https://learning.edx.org/course/course-v1:GoogleCloud+GCCF2x+3T2022/home) course on edX. I'll probably finish it by the end of the week.

### Notes

- Storage options in the Cloud
  - Relational databases
    - Cloud SQL
    - Cloud Spanner
  - NoSQL databases
    - Firestore
    - Bigtable
  - World-wide object storage
- Use cases
  - Content storage and delivery
  - Data analytics and general compute
  - Backup and archival storage
- Structured and unstructured storage
  - Unstructured data
    - Documents
    - Images
    - Audio files
  - Structured data
    - Tables with rows and columns
    - Can be transactional or analytical workloads
      - Transactional workloads involve frequent reads and writes of data to keep records up to date
      - Analytical workloads involve data computation and analysis

| Storage option | SQL or NoSQL | Transactional or analytical | Local or global scalability |
| -------------- | ------------ | --------------------------- | --------------------------- |
| Cloud SQL      | SQL          | Transactional               | Local                       |
| Cloud Spanner  | SQL          | Transactional               | Global                      |
| Firestore      | NoSQL        | Transactional               | /                           |
| BigQuery       | SQL          | Analytical                  | /                           |
| Bigtable       | NoSQL        | Analytical                  | /                           |

## Competitive programming

Today, I worked on some USACO problems:

- [Haybale Feast](http://www.usaco.org/index.php?page=viewproblem2&cpid=767)
- [Lasers and Mirrors](http://www.usaco.org/index.php?page=viewproblem2&cpid=671)

## üî¨ NUS Computer Science Research Week 2023

Today marks the start of the [NUS Computer Science Research Week 2023](https://researchweek.comp.nus.edu.sg/). Here are some notes from the first day.

### Talk 1: The privacy risks of neural networks and how to ensure the models preserve privacy

Note: This talk was given by Prof. [Anastasia Borovykh](https://www.imperial.ac.uk/people/a.borovykh) from Imperial College London.

- As model parameters, output, and gradients are shared in public, anyone can query the model to get potentially sensitive information

  - Attacks
    - Membership inference
      - Given a dataset, can we tell if a particular data point is in the dataset?
      - Can be used to infer sensitive information about a person (e.g. medical condition)
    - Dataset reconstruction
      - Given a model, can we reconstruct the input based on the labels?
      - Formulating an optimization problem: minimize the distance between the reconstructed input and the original input?
    - Attack on the output
      - Adversary can _train_ the model such that the output can reveal additional information about the input (i.e. honest/curious decisions)
  - Measuring leakage (the amount of information leaked by the model)

- How to protect against these attacks?
  - Using more aggregation (larger batch size)
    - This is because the more data points are aggregated, the more difficult it is to exactly reconstruct input from the gradients
  - Restricting an attacker to not be able to modify the training process
  - **Differential privacy!**
    - A randomized algorithm is $(\epsilon, \delta)$-differentially private if for all data sets $D_1$ and $D_2$ such that $Pr[D_1 \neq D_2] \leq \delta$, the following holds:
      - $Pr[\text{algorithm}(D_1) \in A] \leq e^\epsilon Pr[\text{algorithm}(D_2) \in A]$, where $A$ is the set of outputs of the algorithm, $D_1$ and $D_2$ are the two data sets, and $Pr$ is the probability distribution over the data sets.
    - The privacy budget $\epsilon$ is the amount of noise added to the output of the algorithm
    - The risk of stochastic gradient descent
    - Adding anisotropic (i.e. different in different directions) noise to the gradients

### Talk 2: Verifying computations and the robustness of polynomials

Note: This talk was given by Prof. [Swastik Kopparty](https://www.math.toronto.edu/swastik/) from the University of Toronto.

- Delegating computation
  - A client can delegate computation to a server
  - The server can then verify the computation
  - The client can then verify the verification
  - **However, how do we verify the verification?**
- Standard proof systems
  - 3SAT formula (3-Satisfiability)
    - If even one clause is violated, then the whole formula is not satisfiable
  - Finding satisfying assignments to a boolean formula
- Super-fast checkable proofs (PCPs/Probabilistically checkable proofs)
  - Given a 3SAT formula $f$, we can convert a witness w to an "encoded" witness w' such that w' can be checked locally
  - w' has length $poly(f)$
  - **Randomized verification**
  - Impact in theoretical computer science but not in practice
    - PCP witness too long
    - Transformation from w to w' is not efficient
- Interactive proofs
  - Proving with the help of interaction
- Error correcting codes
  - Code
  - Distance property
  - Decoding
  - Widely used for error correction in communication systems
  - Reed-Solomon (univariate) and Reed-Muller (multivariate) codes
- Getting even shorter proof lengths under cryptographic assumptions
  - Using interaction with the prover and then removing it heuristicallyq
  - e.g. SNARKs (succinct non-interactive arguments of knowledge)
- FRI protocol
  - Algebraic sketching
  - Degree respecting sketching (FFT-like)
  - Completeness: If $f$ is low degree and the prover is honest, then it clearly passes
  - Soundness: If $f$ is far from low degree, we want to show that the protocol rejects with high probability
    - **How far is far?** New optimal soundness theorem
    - Leads to shorter STARK proofs
- tl;dr
  - Presenting proofs in an quickly checkable format
  - Proving proximity to (polynomial) codes
  - Many recent advances in the field
  - Use cases and applications
    - Verifying computations in the cloud/blockchain
  - Pushing for practicality

### Talk 3: Cryptographic Lemons (and Lemonades?) in Machine Learning

Note: This talk was given by Prof. [Vinod Vaikuntanathan](https://people.csail.mit.edu/vinodv/) from MIT.

- Machine learning v.s. cryptography
  - Machine learning: x and f(x) are given, find f
  - Cryptography: x is given, find f(x)
    - Psuedorandom functions
- Gaussian mixture learning: Given a set of points, find the parameters of the Gaussian mixture model that best fits the data
  - Density estimation and distinguishing
- Delegation of learning
  - Scientists send data to a service provider
  - Service provider trains a model and sends it back
  - Serious privacy concerns regarding the accuracy, fairness, and robustness of the model
- Adversarial robustness
  - A small perturbation to the input that causes the model to misclassify the input
- **How can the scientist verify that the model is robust?**
- (Continuous) Learning with Errors (CLWE)
  - Homogeneous CLWE $\cong$ a mixture of Gaussians; "Gaussian pancakes"
- Using cryptography to insert an _undetectable_ backdoor in a classifier
  - Can then slightly alter the input to cause the classifier to misclassify and create large deviation in the model output

Unfortunately, I was unable to attend the last talk of the day fully due to another commitment.

## üìö Reading the papers presented today at the Research Week

- [Quantifying and Localizing Usable Information Leakage from Neural Network Gradients](https://arxiv.org/pdf/2105.13929.pdf): Paper presented by Anastasia Borovykh
- [Continuous LWE is as Hard as LWE & Applications to Learning Gaussian Mixtures](https://arxiv.org/pdf/2204.02550.pdf): Paper presented by Vinod Vaikuntanathan
